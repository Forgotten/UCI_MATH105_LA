{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 6\n",
    "\n",
    "We want to solve the linear system given by \n",
    "\\begin{equation}\n",
    "Ax = b,\n",
    "\\end{equation}\n",
    "where $A$ is a non-singular $n\\times n$ matrix. \n",
    "\n",
    "You have already solved this problem using Gaussian Elimination (and its partial pivoted version) which has an assymptotic cost $\\mathcal{O}(n^3)$. \n",
    "\n",
    "In this homework you will try to solve the system in lower complexity using an iterative method. In particular, you will implement the Jacobi and Gauss-Seidel iterations and you will study its limitations.\n",
    "\n",
    "Both Jacobi and Gauss-Iterations can be seen as fixed point methods, used by a matrix splitting. \n",
    "\n",
    "#### Matrix Splitting\n",
    "\n",
    "Given a square matrix $A$ you will define a splitting in two matrices $N$ and $M$ such that $A = N+M$.\n",
    "If you suppose that $N$ is invertible you can write the system \n",
    "\\begin{equation}\n",
    "Ax = b,\n",
    "\\end{equation}\n",
    "as \n",
    "\\begin{equation}\n",
    "Nx =  b - Mx,\n",
    "\\end{equation}\n",
    "and you can define the fixed point iteration as\n",
    "\\begin{equation}\n",
    "Nx^{n+1} =  b - Mx^{n},\n",
    "\\end{equation}\n",
    "or equivalently\n",
    "\\begin{equation}\n",
    "x^{n+1} = N^{-1}\\left ( b - Mx^{n} \\right).\n",
    "\\end{equation}\n",
    "\n",
    "In this case you can show that the convergence speed is can be determined from the spectral radius of the iteration matrix:\n",
    "\\begin{equation}\n",
    "T =  N^{-1}M.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Jacobi Iteration\n",
    "\n",
    "The Jacobi iteration correspond to a fixed point method, in which the Matrix splitting is given by \n",
    "\\begin{equation}\n",
    "N = D, \\qquad M= L+U,\n",
    "\\end{equation}\n",
    "where $D$ is the diagonal of $A$ and $L$ and $U$ are the (strictly) upper and lower triangular parts of $A$.\n",
    "\n",
    "In order to extract the diagonal, upper triangular and lower triangular matrices from $A$ we will use the built-in functions in Julia. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in the splitting = 0.0\n"
     ]
    }
   ],
   "source": [
    "# size of the matrices\n",
    "m = 10\n",
    "# generate a random matrix\n",
    "A = rand(m,m) + m*eye(m)\n",
    "# get the digonal part\n",
    "D = diagm(diag(A),0)\n",
    "# to get the upper triangular part of the matrix\n",
    "U = triu(A,1)\n",
    "#and the lower part \n",
    "L = tril(A,-1)\n",
    "# we check that this is indeed a matrix splitting\n",
    "println(\"Error in the splitting = \",norm(A -(L+D+U)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.a Implement the Jacobi iteration with input:\n",
    "- $A$ a square matrix,\n",
    "- $b$ a vector <br>\n",
    "\n",
    "Your function will have the following optional parameters\n",
    "- $Nmax$ maximum number of iterations, by default set to 30\n",
    "- $\\epsilon$ the tolerance, by default set to 1e-6\n",
    "- history this is a boolean to return all the succesive approximations <br> \n",
    "\n",
    "The ouput of your function is the final approximation $x$ of your vector, of in the case that history is true, it will output a matrix with the all the intermediate approximations of size $ n \\times \\mbox{number of iterations to converge}$.\n",
    "\n",
    "Your function will raise an error if it didn't converge in the $Nmax$ iterations.\n",
    "\n",
    "Hint:\n",
    "- to build the matrix with the intermediat steps you can use hcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JacobiIt (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function JacobiIt(A,b; Nmax = 30, 系=1e-6, history = false)\n",
    "    (n,m) = size(A)\n",
    "    n!=m && error(\"Dimension mistmach\")\n",
    "    # initial guess\n",
    "    x0 = zeros(b)\n",
    "    history && (xHist = x0)\n",
    "    absB = norm(b)\n",
    "    # D\n",
    "    D = diagm(diag(A),0)\n",
    "    # L+U\n",
    "    M = triu(A,1) + tril(A,-1)\n",
    "    for i = 1:Nmax\n",
    "        x = D\\(b - M*x0)\n",
    "        history && (xHist = hcat(xHist,x))\n",
    "        if norm(x-x0)/absB < 系\n",
    "            history ? (return xHist[:,2:end]) : (return x)\n",
    "        end\n",
    "        x0 = x\n",
    "    end\n",
    "    error(\"Tolerance not achieved within the maximum number of iterations\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(m,m) + m*eye(m)\n",
    "b = rand(m,1)\n",
    "X = JacobiIt(A,b, history = true)\n",
    "size(X,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Gauss-Seidel iteration\n",
    "\n",
    "The Gauss-Seidel iteration is analogous to the Jacobi iteration; however, it uses a different splitting, \n",
    "namely \n",
    "\\begin{equation}\n",
    "N = D+U, \\qquad M= L,\n",
    "\\end{equation}\n",
    "\n",
    "Q1.a Implement the Gauss-Seidel iteration with input:\n",
    "- $A$ a square matrix,\n",
    "- $b$ a vector <br>\n",
    "\n",
    "Your function will have the following optional parameters\n",
    "- $Nmax$ maximum number of iterations, by default set to 30\n",
    "- $\\epsilon$ the tolerance, by default set to 1e-6\n",
    "- history this is a boolean to return all the succesive approximations <br> \n",
    "\n",
    "The ouput of your function is the final approximation $x$ of your vector, of in the case that history is true, it will output a matrix with the all the intermediate approximations of size $ n \\times \\mbox{number of iterations to converge}$\n",
    "\n",
    "Your function will raise an error if it didn't converge in the $Nmax$ iterations.\n",
    "\n",
    "Hint:\n",
    "- to build the matrix with the intermediat steps you can use hcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussSeidelIt (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function GaussSeidelIt(A,b; Nmax = 30, 系=1e-6, history = false)\n",
    "    x0 = zeros(b)\n",
    "    history && (xHist = x0)\n",
    "    absB = norm(b)\n",
    "    N = triu(A,0)\n",
    "    # to get the upper triangular part of the matrix\n",
    "    M = tril(A,-1)\n",
    "    for i = 1:Nmax\n",
    "        x = N\\(b - M*x0)\n",
    "        history && (xHist = hcat(xHist,x))\n",
    "        if norm(x-x0)/absB < 系\n",
    "            history ? (return xHist[:,2:end]) : (return x)\n",
    "        end\n",
    "        x0 = x\n",
    "    end\n",
    "    error(\"Tolerance not achieved within the maximum number of iterations\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(m,m) + m*eye(m)\n",
    "b = rand(m,1)\n",
    "X = GaussSeidelIt(A,b, history = true)\n",
    "size(X,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Comparison \n",
    "\n",
    "Use your algorithms to solve the following randomly generated system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.002490 seconds (347 allocations: 506.011 KB)\n",
      "The residual is : 4.240519883754411e-5\n",
      "number of iterations is : 14\n",
      "  0.000487 seconds (99 allocations: 675.609 KB)\n",
      "The residual is : 1.5830214054637808e-6\n",
      "number of iterations is : 6\n"
     ]
    }
   ],
   "source": [
    "m = 100\n",
    "A = rand(m,m) + m*eye(m)\n",
    "b = rand(m,1)\n",
    "\n",
    "@time XJac = JacobiIt(A,b, history = true)\n",
    "println(\"The residual is : \", norm(A*XJac[:,end]- b)/norm(b) )\n",
    "println(\"number of iterations is : \",size(XJac,2))\n",
    "\n",
    "@time XGS = GaussSeidelIt(A,b, history = true)\n",
    "println(\"The residual is : \", norm(A*XGS[:,end]- b)/norm(b) )\n",
    "println(\"number of iterations is : \",size(XGS,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.a How can you explain the one converges faster than the other? Write a small script that computes the spectral radius of the iteration matrix for both algorithms and use it on your explanation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write your script in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will use your algorithms to solve a slighly different system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = 100;\n",
    "A = rand(m,m) + sqrt(m)*eye(m);\n",
    "b = rand(m,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Jacobi iteration, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: Tolerance not achieved within the maximum number of iterations\nwhile loading In[122], in expression starting on line 155",
     "output_type": "error",
     "traceback": [
      "LoadError: Tolerance not achieved within the maximum number of iterations\nwhile loading In[122], in expression starting on line 155",
      "",
      " in JacobiIt at In[88]:16"
     ]
    }
   ],
   "source": [
    "@time XJac = JacobiIt(A,b, history = true)\n",
    "println(\"number of iterations is : \",size(XJac,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and with the Gauss Seidel iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000570 seconds (218 allocations: 1.446 MB)\n",
      "number of iterations is : 15\n"
     ]
    }
   ],
   "source": [
    "@time XGS = GaussSeidelIt(A,b, history = true)\n",
    "println(\"number of iterations is : \",size(XGS,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.b How can you explain the one converges and the other just fails? Write a small script that computes the spectral radius of the iteration matrix for both algorithms and use it on your explanation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.001731157500526"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your script in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Complexity (bonus) (10 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4.a what is the complexity of one iteration of the Gauss-Seidel method? and about Jacobi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4.b What would be the condition on the number of iterations in order such that the Jacobi and Gauss-Seidel iteration would have a better complexity that Gaussian Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4.c Run a benchmark with the randomly generated systems above. Can you achieve quadratic convergence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write your whole script here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.1-pre",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
