{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 6\n",
    "\n",
    "Name: \n",
    "\n",
    "Collaborators:\n",
    "\n",
    "due November 18th, 2015\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We want to solve the linear system given by \n",
    "\\begin{equation}\n",
    "Ax = b,\n",
    "\\end{equation}\n",
    "where $A$ is a non-singular $n\\times n$ matrix. \n",
    "\n",
    "You have already solved this problem using Gaussian Elimination (and its partial pivoted version) which has an assymptotic cost $\\mathcal{O}(n^3)$. \n",
    "\n",
    "In this homework you will try to solve the system in lower complexity using an iterative method. In particular, you will implement the Jacobi and Gauss-Seidel iterations and you will study its limitations.\n",
    "\n",
    "Both Jacobi and Gauss-Iterations can be seen as fixed point methods, used by a matrix splitting. \n",
    "\n",
    "#### Matrix Splitting\n",
    "\n",
    "Given a square matrix $A$ you will define a splitting in two matrices $N$ and $M$ such that $A = N+M$.\n",
    "If you suppose that $N$ is invertible you can write the system \n",
    "\\begin{equation}\n",
    "Ax = b,\n",
    "\\end{equation}\n",
    "as \n",
    "\\begin{equation}\n",
    "Nx =  b - Mx,\n",
    "\\end{equation}\n",
    "and you can define the fixed point iteration as\n",
    "\\begin{equation}\n",
    "Nx^{n+1} =  b - Mx^{n},\n",
    "\\end{equation}\n",
    "or equivalently\n",
    "\\begin{equation}\n",
    "x^{n+1} = N^{-1}\\left ( b - Mx^{n} \\right),\n",
    "\\end{equation}\n",
    "in which the inverse of $N$ is never computed explicitily. \n",
    "\n",
    "In this case you can show that the convergence speed is can be determined from the spectral radius of the iteration matrix:\n",
    "\\begin{equation}\n",
    "T =  N^{-1}M.\n",
    "\\end{equation}\n",
    "\n",
    "#### Convergence rate\n",
    "\n",
    "If we define \n",
    "\\begin{equation}\n",
    "f(x) :=  N^{-1}\\left ( b - Mx^{n} \\right),\n",
    "\\end{equation}\n",
    "then we are solving the fixed point problem \n",
    "\\begin{equation}\n",
    "x = f(x).\n",
    "\\end{equation}\n",
    "\n",
    "We know that in this case, the fixed point iteration given by $x^{n+1} = f(x^n)$ converges if and only if there exist $\\kappa <1$ such that \n",
    "\\begin{equation}\n",
    "\\| f(x) - f(y) \\| \\leq \\kappa \\|x - y\\|, \\qquad \\forall x,y \\in R^n,\n",
    "\\end{equation}\n",
    "where we are using the Euclidean metric given by \n",
    "\\begin{equation}\n",
    "\\| x \\| = \\sqrt{\\sum_{i = 1}^n |x_i|^2 }.\n",
    "\\end{equation}\n",
    "\n",
    "In such case, the error is given by \n",
    "\\begin{equation}\n",
    "\\| x^{n+1} - x\\| \\leq \\frac{\\kappa^{n+1}}{1 - \\kappa} \\| x^{0} - x\\|,\n",
    "\\end{equation}\n",
    "in other words, the convergence and its ratio ar given by $\\kappa$.\n",
    "\n",
    "\n",
    "Using the expression for $f$ we have that\n",
    "\\begin{align}\n",
    "  \\| f(x) - f(y) \\| & = \\| N^{-1}(b - Mx) - N^{-1}(b - My)  \\|, \\\\\n",
    "                    & = \\| N^{-1} M(x -y)  \\|, \\\\\n",
    "                    & \\leq \\|N^{-1} M \\| \\|x - y \\|;\n",
    "\\end{align}\n",
    "or \n",
    "\\begin{equation}\n",
    "\\kappa = \\|N^{-1} M \\| = \\|T \\|.\n",
    "\\end{equation}\n",
    "\n",
    "When we are using the Euclidean norm, we have that \n",
    "\\begin{equation}\n",
    "\\|T \\| = \\max_{z \\neq 0} \\frac{\\|Tz \\|}{\\| z\\|} = \\rho(T).\n",
    "\\end{equation}\n",
    "where the spectral radius $\\rho(T)$ is given by \n",
    "\\begin{equation}\n",
    "\\rho(T) = \\max_{i = 1,..,n} |\\lambda_i|, \\qquad \\mbox{ where } \\lambda_i \\mbox{ are the eigenvalues of } T\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Jacobi Iteration\n",
    "\n",
    "The Jacobi iteration correspond to a fixed point method, in which the Matrix splitting is given by \n",
    "\\begin{equation}\n",
    "N = D, \\qquad M= L+U,\n",
    "\\end{equation}\n",
    "where $D$ is the diagonal of $A$ and $L$ and $U$ are the (strictly) upper and lower triangular parts of $A$.\n",
    "\n",
    "In order to extract the diagonal, upper triangular and lower triangular matrices from $A$ we will use the built-in functions in Julia. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in the splitting = 0.0\n"
     ]
    }
   ],
   "source": [
    "# size of the matrices\n",
    "m = 10\n",
    "# generate a random matrix\n",
    "A = rand(m,m) + m*eye(m)\n",
    "# get the digonal part\n",
    "D = diagm(diag(A),0)\n",
    "# to get the upper triangular part of the matrix\n",
    "U = triu(A,1)\n",
    "#and the lower part \n",
    "L = tril(A,-1)\n",
    "# we check that this is indeed a matrix splitting\n",
    "println(\"Error in the splitting = \",norm(A -(L+D+U)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.a Implement the Jacobi iteration with input:\n",
    "- $A$ a square matrix,\n",
    "- $b$ a vector <br>\n",
    "\n",
    "Your function will have the following optional parameters\n",
    "- $Nmax$ maximum number of iterations, by default set to 30\n",
    "- $\\epsilon$ the tolerance, by default set to 1e-6\n",
    "- history this is a boolean to return all the succesive approximations <br> \n",
    "\n",
    "The ouput of your function is the final approximation $x$ of your vector, of in the case that history is true, it will output a matrix with the all the intermediate approximations of size $ n \\times \\mbox{number of iterations to converge}$.\n",
    "\n",
    "Your function will raise an error if it didn't converge in the $Nmax$ iterations.\n",
    "\n",
    "Hint:\n",
    "- to build the matrix with the intermediat steps you can use hcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JacobiIt (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function JacobiIt(A,b; Nmax = 30, ϵ=1e-6, history = false)\n",
    "    (n,m) = size(A)\n",
    "    n!=m && error(\"Dimension mistmach\")\n",
    "    # initial guess\n",
    "    x0 = zeros(b)\n",
    "    history && (xHist = x0)\n",
    "    absB = norm(b)\n",
    "    # D\n",
    "    D = diagm(diag(A),0)\n",
    "    # L+U\n",
    "    M = triu(A,1) + tril(A,-1)\n",
    "    for i = 1:Nmax\n",
    "        x = D\\(b - M*x0)\n",
    "        history && (xHist = hcat(xHist,x))\n",
    "        if norm(x-x0)/absB < ϵ\n",
    "            history ? (return xHist[:,2:end]) : (return x)\n",
    "        end\n",
    "        x0 = x\n",
    "    end\n",
    "    error(\"Tolerance not achieved within the maximum number of iterations\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Gauss-Seidel iteration\n",
    "\n",
    "The Gauss-Seidel iteration is analogous to the Jacobi iteration; however, it uses a different splitting, \n",
    "namely \n",
    "\\begin{equation}\n",
    "N = D+U, \\qquad M= L,\n",
    "\\end{equation}\n",
    "\n",
    "Q1.a Implement the Gauss-Seidel iteration with input:\n",
    "- $A$ a square matrix,\n",
    "- $b$ a vector <br>\n",
    "\n",
    "Your function will have the following optional parameters\n",
    "- $Nmax$ maximum number of iterations, by default set to 30\n",
    "- $\\epsilon$ the tolerance, by default set to 1e-6\n",
    "- history this is a boolean to return all the succesive approximations <br> \n",
    "\n",
    "The ouput of your function is the final approximation $x$ of your vector, of in the case that history is true, it will output a matrix with the all the intermediate approximations of size $ n \\times \\mbox{number of iterations to converge}$\n",
    "\n",
    "Your function will raise an error if it didn't converge in the $Nmax$ iterations, and if the dimension of $A$ and $b$ are not consistent.\n",
    "\n",
    "Hint:\n",
    "- to build the matrix with the intermediat steps you can use hcat\n",
    "- you can type ? hcat to get the built-in help\n",
    "- you won't compute the inverse inv(N) explicitly, you will use a triangular solver by \"\\\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussSeidelIt (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function GaussSeidelIt(A,b; Nmax = 30, ϵ=1e-6, history = false)\n",
    "    x0 = zeros(b)\n",
    "    history && (xHist = x0)\n",
    "    absB = norm(b)\n",
    "    N = triu(A,0)\n",
    "    # to get the upper triangular part of the matrix\n",
    "    M = tril(A,-1)\n",
    "    for i = 1:Nmax\n",
    "        x = N\\(b - M*x0)\n",
    "        history && (xHist = hcat(xHist,x))\n",
    "        if norm(x-x0)/absB < ϵ\n",
    "            history ? (return xHist[:,2:end]) : (return x)\n",
    "        end\n",
    "        x0 = x\n",
    "    end\n",
    "    error(\"Tolerance not achieved within the maximum number of iterations\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Comparison \n",
    "\n",
    "Use your algorithms to solve the following randomly generated system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.328955 seconds (661.30 k allocations: 31.605 MB, 2.90% gc time)\n",
      "The residual is : 3"
     ]
    }
   ],
   "source": [
    "m = 100\n",
    "A = rand(m,m) + m*eye(m)\n",
    "b = rand(m,1)\n",
    "\n",
    "@time XJac = JacobiIt(A,b, history = true)\n",
    "println(\"The residual is : \", norm(A*XJac[:,end]- b)/norm(b) )\n",
    "println(\"number of iterations is : \",size(XJac,2))\n",
    "\n",
    "@time XGS = GaussSeidelIt(A,b, history = true)\n",
    "println(\"The residual is : \", norm(A*XGS[:,end]- b)/norm(b) )\n",
    "println(\"number of iterations is : \",size(XGS,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.a How can you explain the one converges faster than the other? Write a small script that computes the spectral radius of the iteration matrix for both algorithms and use it on your explanation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".915839178750226e-5\n",
      "number of iterations is : 14\n",
      "  0.026476 seconds (36.29 k allocations: 1.872 MB)\n",
      "The residual is : 1.3149070138855238e-6\n",
      "number of iterations is : 6\n",
      "Spectral radius of the iteration of the Jacobi iteration is 0.4905944402067775\n",
      "Spectral radius of the iteration of the Jacobi iteration is 0.10284071798268372\n"
     ]
    }
   ],
   "source": [
    "# we compute the spectral radius of the iteration matrix for both method\n",
    "# Jacobi\n",
    "\n",
    "N = diagm(diag(A),0)\n",
    "M  = triu(A,1)+ tril(A,-1)\n",
    "\n",
    "TJac = N\\M;\n",
    "λ = eigvals(TJac)\n",
    "println(\"Spectral radius of the iteration of the Jacobi iteration is \", maximum(abs(λ)));\n",
    "\n",
    "# Gauss Seidel\n",
    "N = triu(A,0)\n",
    "M  = tril(A,-1)\n",
    "\n",
    "TGS = N\\M;\n",
    "λ = eigvals(TGS)\n",
    "println(\"Spectral radius of the iteration of the Jacobi iteration is \", maximum(abs(λ)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The number of iteration to converge is determined by the spectral radius of the iteration matrix. A smaller radius will mean a faster convergence, as you can see above, the spectral radius in the case of the Gauss Seidel iteration is much smaller than the one for Jacobi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will use your algorithms to solve a slighly different system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = 100;\n",
    "A = rand(m,m) + sqrt(m)*eye(m);\n",
    "b = rand(m,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Jacobi iteration, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: Tolerance not achieved within the maximum number of iterations\nwhile loading In[9], in expression starting on line 155",
     "output_type": "error",
     "traceback": [
      "LoadError: Tolerance not achieved within the maximum number of iterations\nwhile loading In[9], in expression starting on line 155",
      "",
      " in JacobiIt at In[4]:20"
     ]
    }
   ],
   "source": [
    "@time XJac = JacobiIt(A,b, history = true)\n",
    "println(\"number of iterations is : \",size(XJac,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and with the Gauss Seidel iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.001056 seconds (391 allocations: 599.203 KB)\n",
      "number of iterations is : 27\n"
     ]
    }
   ],
   "source": [
    "@time XGS = GaussSeidelIt(A,b,Nmax = 60, history = true)\n",
    "println(\"number of iterations is : \",size(XGS,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.b How can you explain the one converges and the other just fails? Write a small script that computes the spectral radius of the iteration matrix for both algorithms and use it on your explanation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral radius of the iteration of the Jacobi iteration is 4.695089018249572\n",
      "Spectral radius of the iteration of the Jacobi iteration is 0.6846425465044048\n"
     ]
    }
   ],
   "source": [
    "# we compute the spectral radius of the iteration matrix for both method\n",
    "# Jacobi\n",
    "\n",
    "N = diagm(diag(A),0)\n",
    "M  = triu(A,1)+ tril(A,-1)\n",
    "\n",
    "TJac = N\\M;\n",
    "λ = eigvals(TJac)\n",
    "println(\"Spectral radius of the iteration of the Jacobi iteration is \", maximum(abs(λ)));\n",
    "\n",
    "# Gauss Seidel\n",
    "N = triu(A,0)\n",
    "M  = tril(A,-1)\n",
    "\n",
    "TGS = N\\M;\n",
    "λ = eigvals(TGS)\n",
    "println(\"Spectral radius of the iteration of the Jacobi iteration is \", maximum(abs(λ)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: We can observe that Jacobi iteration diverges, this is because the spectral radius of the iteration matrix is greater than 1, meaning that it has a instable mode. On the other hand, the spectral radius of the iteration matrix for Gauss-Seidel is well below one, meaning that the method converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Complexity (bonus) (10 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4.a what is the complexity of one iteration of the Gauss-Seidel method? and about Jacobi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4.b What would be the condition on the number of iterations in order such that the Jacobi and Gauss-Seidel iteration would have a better complexity that Gaussian Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4.c Run a benchmark with the randomly generated systems above. Can you achieve quadratic convergence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write your whole script here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2-pre",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
